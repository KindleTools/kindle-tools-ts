# Architecture

Technical overview of kindle-tools-ts for contributors and developers who want to understand or extend the codebase.

> **Note:** This is a **pure TypeScript library**. The CLI has been removed. A visual workbench for testing is available in `tests/workbench/`.

## Project Structure

```
kindle-tools-ts/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/               # Orchestration & System-wide logic
â”‚   â”‚   â”œâ”€â”€ constants.ts    # System constants
â”‚   â”‚   â”œâ”€â”€ processor.ts    # Dedup, merge, link notes (The "Processor")
â”‚   â”‚   â””â”€â”€ processing/     # Processing modules (dedup, merge, link, quality)
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/             # Pure Business Logic (Entities & Rules)
â”‚   â”‚   â”œâ”€â”€ index.ts        # Barrel export for all domain modules
â”‚   â”‚   â”œâ”€â”€ stats.ts        # Statistics logic
â”‚   â”‚   â”œâ”€â”€ geography.ts    # Coordinates & Distance
â”‚   â”‚   â”œâ”€â”€ tags.ts         # Business rules for cleaning tags
â”‚   â”‚   â”œâ”€â”€ locations.ts    # Page/location utilities
â”‚   â”‚   â””â”€â”€ sanitizers.ts   # Title/Author cleaning rules
â”‚   â”‚
â”‚   â”œâ”€â”€ importers/          # Data Ingestion
â”‚   â”‚   â”œâ”€â”€ index.ts        # Barrel export
â”‚   â”‚   â”œâ”€â”€ core/           # Factory & types
â”‚   â”‚   â”œâ”€â”€ formats/        # Concrete implementations (txt, json, csv)
â”‚   â”‚   â”‚   â””â”€â”€ txt/        # Kindle TXT parser (tokenizer, parser, language detector)
â”‚   â”‚   â””â”€â”€ shared/         # Base classes & utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ exporters/          # Export Adapters
â”‚   â”‚   â”œâ”€â”€ index.ts        # Barrel export
â”‚   â”‚   â”œâ”€â”€ core/           # Factory & types
â”‚   â”‚   â”œâ”€â”€ formats/        # Concrete implementations (json, csv, md, obsidian, joplin, html)
â”‚   â”‚   â””â”€â”€ shared/         # Base classes & utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/              # Generic, App-Agnostic Utilities
â”‚   â”‚   â”œâ”€â”€ text/           # Normalizers, patterns, similarity, encoding
â”‚   â”‚   â”œâ”€â”€ fs/             # Archive creation (tar, zip)
â”‚   â”‚   â”œâ”€â”€ system/         # Dates, errors
â”‚   â”‚   â””â”€â”€ security/       # CSV sanitizer
â”‚   â”‚
â”‚   â”œâ”€â”€ types/              # Shared Types
â”‚   â”œâ”€â”€ schemas/            # Zod validation schemas
â”‚   â”œâ”€â”€ templates/          # Handlebars templates
â”‚   â”œâ”€â”€ plugins/            # Plugin system (registry, hooks, discovery)
â”‚   â”œâ”€â”€ errors/             # Error handling (neverthrow Result types)
â”‚   â”œâ”€â”€ config/             # Configuration loading (cosmiconfig)
â”‚   â”œâ”€â”€ node/               # Node.js-specific entry point (parseFile)
â”‚   â””â”€â”€ index.ts            # Public API (browser-safe)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â”œâ”€â”€ integration/        # Pipeline tests
â”‚   â”œâ”€â”€ fixtures/           # Test data
â”‚   â””â”€â”€ workbench/          # Visual testing GUI (Vite app)
â”‚
â””â”€â”€ dist/                   # Build output
```

## Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        My Clippings.txt                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. TOKENIZER                                                        â”‚
â”‚     - Split by "==========" separator                                â”‚
â”‚     - Normalize line endings                                         â”‚
â”‚     - Remove BOM                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. LANGUAGE DETECTOR                                                â”‚
â”‚     - Analyze metadata patterns ("Added on", "AÃ±adido el", etc.)     â”‚
â”‚     - Support: EN, ES, PT, DE, FR, IT, ZH, JA, KO, NL, RU            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. PARSER                                                           â”‚
â”‚     - Extract: title, author, type, page, location, date, content    â”‚
â”‚     - Generate deterministic ID (SHA-256 truncated)                  â”‚
â”‚     - Detect DRM limit messages                                      â”‚
â”‚     - Identify sideloaded books (.pdf, .epub)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. PROCESSOR (optional steps)                                       â”‚
â”‚     â”œâ”€ removeDuplicates()     â†’ Exact duplicate removal              â”‚
â”‚     â”œâ”€ smartMergeHighlights() â†’ Merge overlapping selections         â”‚
â”‚     â”œâ”€ linkNotesToHighlights()â†’ Associate notes with highlights      â”‚
â”‚     â”œâ”€ extractTagsFromLinkedNotes() â†’ Parse #tags from notes         â”‚
â”‚     â”œâ”€ flagSuspiciousHighlights()   â†’ Mark accidental/fragments      â”‚
â”‚     â””â”€ flagFuzzyDuplicates()  â†’ Jaccard similarity detection         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. EXPORTER                                                         â”‚
â”‚     - JSON, CSV, Markdown, Obsidian, Joplin JEX, HTML                â”‚
â”‚     - Configurable folder structure and author case                  â”‚
â”‚     - Tag synchronization across formats                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Model

### Clipping

The core data structure representing a single clipping:

```typescript
interface Clipping {
  // Identification
  id: string;                    // Deterministic SHA-256 hash (12 chars)
  blockIndex: number;            // Original position in file

  // Book metadata
  title: string;                 // Clean title
  titleRaw: string;              // Original title (preserved)
  author: string;                // Extracted author
  authorRaw: string;             // Original author (preserved)

  // Content
  content: string;               // Clean content
  contentRaw: string;            // Original content (preserved)
  type: 'highlight' | 'note' | 'bookmark' | 'clip' | 'article';

  // Location
  page: number | null;
  location: {
    raw: string;                 // "100-105"
    start: number;
    end: number | null;
  };

  // Date
  date: Date | null;
  dateRaw: string;               // Original date string

  // Flags
  isLimitReached: boolean;       // DRM limit detected
  isEmpty: boolean;              // No content
  language: SupportedLanguage;
  source: 'kindle' | 'sideload';

  // Stats
  wordCount: number;
  charCount: number;

  // Linking (after processing)
  linkedNoteId?: string;         // ID of linked note
  linkedHighlightId?: string;    // ID of linked highlight
  note?: string;                 // Merged note content
  tags?: string[];               // Extracted tags
}
```

## Key Algorithms

### Deterministic ID Generation

IDs are generated using SHA-256 hash of normalized content, ensuring:
- Same clipping always gets same ID
- Idempotent imports (no duplicates on re-import)
- Collision-resistant (12-char hex prefix)

```typescript
const id = sha256(`${title}|${author}|${type}|${location.raw}|${contentPrefix}`).slice(0, 12);
```

### Smart Merge (Overlapping Highlights)

When you extend a highlight in Kindle, it creates a new entry. Smart merge detects these overlaps:

```
Original:  "The quick brown"
Extended:  "The quick brown fox jumps"
Result:    Keep the longer version, merge tags
```

Detection criteria:
- Same title
- Overlapping location ranges
- One content is subset of the other (or high Jaccard similarity)

### Note-to-Highlight Linking

Notes in Kindle are stored separately. Linking associates them:

1. **Range Coverage** (preferred): Note location within highlight's location range
   ```
   Highlight: locations 100-150
   Note: location 120 â†’ LINKED
   ```

2. **Proximity Fallback**: Distance â‰¤ 10 locations
   ```
   Highlight: location 100
   Note: location 105 â†’ LINKED (within 10)
   ```

### Tag Extraction

Tags are extracted from note content:
- Hashtag format: `#important #review`
- Comma/semicolon separated: `important, review`
- Newline separated

After extraction:
- Tags are added to the linked highlight's `tags` array
- Original note content is preserved in `note` field

## Joplin JEX Format

The JEX format is a TAR archive containing:

```
clippings.jex/
â”œâ”€â”€ {id}.md              # Root notebook
â”œâ”€â”€ {id}.md              # Author notebook (optional)
â”œâ”€â”€ {id}.md              # Book notebook
â”œâ”€â”€ {id}.md              # Note (clipping)
â”œâ”€â”€ {id}.md              # Tag
â””â”€â”€ {id}.md              # Note-Tag association
```

Each file has YAML-like metadata at the bottom:
```markdown
[0042] ðŸ“– Quote content here...

**Book:** Title
**Author:** Author Name

---

> The actual highlight text

id: {32-char-deterministic-id}
parent_id: {notebook-id}
created_time: 2024-01-01T00:00:00.000Z
type_: 1  # 1=note, 2=folder, 5=tag, 6=note_tag
```

Deterministic IDs ensure:
- Re-importing updates existing notes instead of creating duplicates
- Consistent hierarchy across imports

## Design Decisions

### Why SHA-256 for IDs?

- **Deterministic**: Same input always produces same output
- **Collision-resistant**: 12 hex chars = 48 bits = very low collision probability
- **Cross-platform**: Works identically in Node.js and browser

### Why Preserve Raw Fields?

The `titleRaw`, `authorRaw`, `contentRaw` fields preserve original data because:
- User can verify cleaning/normalization was correct
- Debugging parsing issues
- Potential future re-processing with different options

### Why Tags in Frontmatter (Obsidian)?

Following Obsidian best practices:
- Tags in YAML frontmatter are indexed by Obsidian
- No hashtags in YAML (they make tags invalid)
- Merged with default tags (`kindle`, `highlights`)

### Why 3-Level Hierarchy for Joplin?

`Root > Author > Book > Notes` mirrors typical library organization:
- Users with many books can navigate by author first
- Configurable: can disable author level for flat structure

## Testing Strategy

```
tests/
â”œâ”€â”€ unit/                    # Isolated function tests
â”‚   â”œâ”€â”€ tokenizer.test.ts    # Block splitting
â”‚   â”œâ”€â”€ parser.test.ts       # Metadata extraction
â”‚   â”œâ”€â”€ processor.test.ts    # Dedup, merge, link
â”‚   â”œâ”€â”€ exporters.test.ts    # All export formats
â”‚   â”œâ”€â”€ normalizers.test.ts  # Unicode, BOM
â”‚   â”œâ”€â”€ sanitizers.test.ts   # Title/author cleaning
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ integration/             # Full pipeline tests
â”‚   â””â”€â”€ pipeline.test.ts     # Parse â†’ Process â†’ Export
â”‚
â””â”€â”€ fixtures/
    â””â”€â”€ sample-clippings.ts  # Test data
```

**Coverage targets**: 80% for lines, functions, branches, statements

## Browser Compatibility

The library is isomorphic (works in Node.js and browser):

- **Node.js**: File system access for `parseFile()` (import from `kindle-tools-ts/node`)
- **Browser**: String parsing only via `parseString()` (import from `kindle-tools-ts`)

The workbench (`tests/workbench/`) uses Vite for development and demonstrates browser usage.

## Error Handling & Result Pattern

The project uses **neverthrow** for type-safe error handling, avoiding try-catch blocks in favor of explicit Result types.

### Result Types

```typescript
// src/errors/result.ts
import { Result, ResultAsync, ok, err } from 'neverthrow';

// Import operations return typed results
type ImportResult = Result<ImportSuccess, ImportError>;
type ExportResult = Result<ExportSuccess, ExportError>;

// Factory functions for consistent error creation
importSuccess(clippings, warnings);
importParseError(message, { path, line, cause });
exportSuccess(output, files);
exportUnknownFormat(format);
```

### Centralized Error Codes

```typescript
// src/errors/codes.ts
export const ErrorCode = {
  // Import errors
  IMPORT_PARSE_ERROR: 'IMPORT_PARSE_ERROR',
  IMPORT_EMPTY_FILE: 'IMPORT_EMPTY_FILE',
  IMPORT_INVALID_FORMAT: 'IMPORT_INVALID_FORMAT',

  // Export errors
  EXPORT_UNKNOWN_FORMAT: 'EXPORT_UNKNOWN_FORMAT',
  EXPORT_WRITE_FAILED: 'EXPORT_WRITE_FAILED',
  EXPORT_TEMPLATE_ERROR: 'EXPORT_TEMPLATE_ERROR',
  // ...
} as const;
```

### Pattern Matching Usage

```typescript
// In CLI and GUI
const result = await importer.import(content);

result.match(
  (success) => {
    console.log(`Imported ${success.clippings.length} clippings`);
    if (success.warnings.length > 0) {
      console.warn('Warnings:', success.warnings);
    }
  },
  (error) => {
    console.error(`[${error.code}] ${error.message}`);
  }
);
```

### Why neverthrow?

- **Type-safe errors**: Errors are part of the function signature
- **Explicit handling**: Cannot forget to handle errors
- **Composable**: Chain operations with `.andThen()`, `.map()`, `.orElse()`
- **Async support**: `ResultAsync` wraps Promise-based operations

## Plugin System

The project includes an extensible plugin architecture for custom importers/exporters.

### Structure

```
src/plugins/
â”œâ”€â”€ types.ts       # Plugin interfaces (ImporterPlugin, ExporterPlugin)
â”œâ”€â”€ registry.ts    # PluginRegistry singleton
â”œâ”€â”€ hooks.ts       # Lifecycle hooks (beforeImport, afterExport, etc.)
â”œâ”€â”€ adapters.ts    # Integration with Factory pattern
â”œâ”€â”€ discovery.ts   # Auto-discovery from node_modules
â””â”€â”€ examples/      # Reference implementations (Anki exporter)
```

### Plugin Interface

```typescript
interface ExporterPlugin {
  name: string;           // Unique identifier (kebab-case)
  version: string;        // Semver
  format: string;         // CLI format name
  aliases?: string[];     // Alternative names
  description?: string;
  create: () => Exporter; // Factory function
}
```

### Lifecycle Hooks

```typescript
hookRegistry.add('beforeExport', (clippings, format) => {
  return clippings.filter(c => c.content.length > 20);
});

hookRegistry.add('afterExport', (output, format) => {
  return `<!-- Generated by KindleTools -->\n${output}`;
});
```

## Schema Validation (Zod)

All external data is validated at system boundaries using **Zod** schemas.

### Schema Organization

```
src/schemas/
â”œâ”€â”€ clipping.schema.ts   # ClippingImportSchema, ClippingStrictSchema
â”œâ”€â”€ config.schema.ts     # ConfigFileSchema, ParseOptionsSchema
â”œâ”€â”€ exporter.schema.ts   # ExporterOptionsSchema
â”œâ”€â”€ cli.schema.ts        # CLI argument validation
â””â”€â”€ index.ts             # Re-exports
```

### Lenient vs Strict Schemas

```typescript
// Lenient: For importing external data (all fields optional)
const ClippingImportSchema = z.object({
  title: z.string().optional(),
  content: z.string().optional(),
  // ...
});

// Strict: For internal validation (required fields enforced)
const ClippingStrictSchema = z.object({
  id: z.string().min(1),
  title: z.string().min(1),
  // ...
});
```

## Dependencies

**Runtime**:
- `date-fns`: Multi-locale date parsing
- `handlebars`: Template engine for exports
- `jszip`: Archive creation (Joplin JEX, etc.)
- `zod`: Schema validation
- `neverthrow`: Type-safe error handling
- `cosmiconfig`: Configuration file discovery

**Development**:
- `tsup`: Build (ESM + CJS + DTS)
- `vitest`: Testing
- `biome`: Linting/formatting
- `husky`: Git hooks
- `changesets`: Versioning

### Architecture Principles (Refactor 2026)

This project strictly adheres to Clean Architecture principles to ensure scalability and maintainability.

#### 1. Factory Pattern & Dependency Injection
We use **Factory Method** patterns to instantiate Importers and Exporters. This decouples the CLI/Consumer from concrete implementations.

- **ImporterFactory:** Dynamically selects the correct parser based on file signature/extension.
- **ExporterFactory:** Provides the requested `Exporter` implementation based on the format string.

#### 2. Domain-Driven Design & Clean Architecture
The project is structured to separate "Business Rules" from "Infrastructure" and "Orchestration".

- **`@domain/*` (The "What"):** Contains pure business logic and rules (e.g., *How do we calculate reading stats?*, *What defines a valid tag?*). These modules are completely independent of the rest of the system.
- **`@core/*` (The "How"):** The application layer that orchestrates the data flow. It uses the Domain to process data but doesn't define the rules itself.
- **`@utils/*` (The "Tools"):** Generic, app-agnostic utilities (e.g., *How to zip a file?*, *How to hash a string?*).

#### 3. Strict Layer Boundaries (Path Aliases)
We enforce structure via TypeScript path aliases:

- `#domain/*`: **Pure Business Logic**. No dependencies on Core or Importers.
- `#core/*`: **Orchestration Logic**. Connects Domain, Importers, and Exporters.
- `#importers/*`: **Adapters** for external data formats.
- `#exporters/*`: **Adapters** for output formats.
- `#utils/*`: **Generic Tools**. Zero dependencies on app logic.
- `#app-types/*`: **Shared Domain Types**.

#### 4. Unified Processing Pipeline
Regardless of the input source (Raw Text, JSON backup, CSV), all data flows through the same **Pipeline**:
1. **Import** (Normalize into `Clipping[]`)
2. **Process** (Deduplicate, Merge, Link Notes) -> *Centralized in `@core/processor`*
3. **Export** (Transform into target format)

This ensures that a JSON backup, when re-imported, undergoes the same rigorous cleanup and deduplication as a raw Kindle file.

### Toolchain & DevOps Strategy (2026 Standards)

The project leverages a "Bleeding Edge, Yet Stable" stack to maximize developer velocity and correctness.

#### 1. High-Performance Tooling
We prioritize tools written in Rust/Go for sub-second feedback loops:
- **Biome:** Replaces ESLint + Prettier. 20x faster, zero-config, handles formatting and linting in one pass.
- **Tsup (esbuild):** Instant builds for the library. Zero config required for ESM/CJS interop.
- **Vitest:** Instant test suite execution with Vite integration (replacing Jest).
- **Turborepo:** Intelligent remote caching. If you only touch the GUI, the Library tests won't re-run in CI.

#### 2. Strict Type Safety
- **Strict Mode:** Enabled in `tsconfig`.
- **`arethetypeswrong`:** Validates `package.json` exports to prevent "it works on my machine" issues for consumers using different module resolution strategies (NodeNext, Bundler, etc.).

#### 3. Monorepo-Lite Structure
Even though it's a single library, we treat it as a monorepo (Library + GUI + Docs) using **pnpm workspaces** + **Turbo**. This allows us to scale the "ecosystem" (e.g. adding a VSCode extension or a Web App later) without restructuring the repo.

#### 4. Import Strategy: Native Node Subpath Imports
To ensure rock-solid compatibility with modern Node.js ESM standards (`NodeNext`), we strictly avoid fragile `tsconfig` `paths`. Instead, we use **Native Node Subpath Imports** (`#alias` prefix).

- **Mechanism:** Mapped in `package.json` under `"imports"`.
  ```json
  "imports": {
    "#core/*.js": "./src/core/*.ts"
  }
  ```
- **Benefit:** This is supported natively by Node.js, and correctly resolved by TypeScript, Esbuild, and Vite without relying on brittle build-time path transformations. It guarantees that our internal aliases behave identically in dev, test, and production builds.



## ðŸ—ï¸ Project Structure

This project follows a **Feature/Domain-based Architecture**, moving away from a traditional Layered Architecture (controllers/services/utils) to better reflect the business domain.

### Core Principles

1.  **Collocation**: Code that changes together stays together.
2.  **Explicit Domain**: The `core` folder contains the business rules, not generic utilities.
3.  **Dependency Rule**: Dependencies point inwards. Importers depend on Core; Core depends on nothing (or minimal Utils).

### Directory Brekadown

```
src/
â”œâ”€â”€ core/                 # ðŸ§  The Brain: Domain Logic (Business Rules)
â”‚   â”œâ”€â”€ processor.ts      # Main pipeline orchestration
â”‚   â”œâ”€â”€ hashing.ts        # ID generation logic (Business Rule)
â”‚   â”œâ”€â”€ similarity.ts     # Fuzzy duplicate detection logic (Business Rule)
â”‚   â””â”€â”€ constants.ts      # Domain constants
â”‚
â”œâ”€â”€ domain/               # ðŸ“¦ Domain Entities & Pure Logic
â”‚   â”œâ”€â”€ stats.ts          # Statistics calculation
â”‚   â”œâ”€â”€ sanitizers.ts     # Data cleaning rules
â”‚   â”œâ”€â”€ languages.ts      # Language definitions & patterns
â”‚   â””â”€â”€ geography.ts      # Geolocation extensions
â”‚
â”œâ”€â”€ importers/            # ðŸ“¥ Data Ingestion
â”‚   â”œâ”€â”€ core/             # Base interfaces and factories
â”‚   â”œâ”€â”€ formats/          # Concrete implementation (txt, json, csv)
â”‚   â””â”€â”€ shared/           # Shared utilities
â”‚
â”œâ”€â”€ exporters/            # ðŸ“¤ Data Egress
â”‚   â”œâ”€â”€ core/             # Base interfaces and factories
â”‚   â”œâ”€â”€ formats/          # Concrete implementations (md, json, joplin...)
â”‚   â””â”€â”€ shared/           # Shared utilities
â”‚
â”œâ”€â”€ utils/                # ðŸ› ï¸ Generic Tools (Stateless & Dumb)
â”‚   â”œâ”€â”€ fs/               # File system helpers (ZIP, TAR)
â”‚   â”œâ”€â”€ system/           # Date formatting, Error handling
â”‚   â””â”€â”€ text/             # Basic string manip (Encoding, Normalizing)
â”‚
â”œâ”€â”€ templates/            # ðŸŽ¨ Export Templates (Handlebars)
â”‚   â””â”€â”€ default.ts        # Default layouts
â”‚
â”œâ”€â”€ node/                 # ðŸŸ¢ Node.js Entry Point
â”‚   â””â”€â”€ index.ts          # Node-specific exports (FileSystem access)
â”‚
â”œâ”€â”€ plugins/              # ðŸ”Œ Plugin System
â”‚   â””â”€â”€ registry.ts       # Plugin registry, hooks, discovery
â”‚
â””â”€â”€ errors/               # âš ï¸ Error Handling (neverthrow)
    â””â”€â”€ result.ts         # Result types and factory functions
```

> **Note:** The visual workbench for testing is at `tests/workbench/main.ts` (not in src/).

---

## ðŸ§  Key Design Decisions

### 1. Hashing as a Domain Concept
**Decision**: Moved `hashing.ts` from `utils` to `core`.
**Reason**: The generation of a Clipping ID is not a generic utility. It is a fundamental business rule that defines **identity**. If we change how we hash (e.g., stopping the inclusion of location), we fundamentally change the application's behavior regarding duplicates.

### 2. Similarity Engine location
**Decision**: Moved `similarity.ts` from `utils` to `core`.
**Reason**: The Jaccard similarity logic is the engine behind the "Fuzzy Duplicate Detection" feature. It is core business logic, not a generic string utility.

### 3. Parser-Specific Cleaning
**Decision**: Moved `text-cleaner.ts` inside `importers/txt/core`.
**Reason**: This file handles artifacts specific to the Kindle TXT format (PDF line breaks, etc.). It is not a general text utility and shouldn't be exposed as such. It is highly coupled to the TXT parser.

### 4. Universal Core vs Node Adapters
**Decision**: Split `src/index.ts` (Universal) and `src/node/index.ts` (Node.js).
**Reason**: To ensure the library works in strictly non-Node environments (like Browsers or Edge Workers), the main entry point MUST NOT depend on 'fs' or 'path'. Node-specific functionality (like `parseFile`) is isolated in a separate entry point.

### 5. Languages as a Domain Concept
**Decision**: Moved `languages.ts` to `domain`.
**Reason**:  Knowledge about what languages are supported and their specific patterns (date formats, keywords) is a core part of the "Kindle Domain", not just an implementation detail of the text parser. This decoupling allows other parts of the system to be language-aware without depending on the `txt` importer.

### 6. Importer/Exporter Pattern
**Decision**: Use a Strategy Pattern for Importers and Exporters.
**Reason**: Allows easy extension. New formats (e.g., Readwise CSV, Notion API) can be added as new classes without modifying the core processing pipeline.

### 7. Deterministic Processing
**Decision**: The `process()` function in `core/processor.ts` is pure and deterministic.
**Reason**: It takes raw clippings and returns processed clippings + stats. It does not perform I/O. This makes the core logic 100% testable without mocks.

---

## ðŸ”„ Data Flow

1.  **Import**: Raw file -> `Importer` -> `Clipping[]` (Raw)
2.  **Process**: `Clipping[]` -> `Processor` -> `Clipping[]` (Cleaned, Deduped, Linked)
3.  **Export**: `Clipping[]` -> `Exporter` -> Output (File/String/Archive)

---

## ðŸ”® Future Improvements

-   **Pipeline Object**: Convert the `process()` function into a composable `Pipeline` class to allow plugins/middleware.
-   **FileSystem Abstraction**: Create a proper `VirtualFileSystem` to handle exports uniformly (whether writing to disk in Node or creating a ZIP in Browser).
